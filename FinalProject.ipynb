{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
      "\n",
      "  Please upgrade your code to TensorFlow 2.0:\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
      "\n",
      "  Or install the latest stable TensorFlow 1.X release:\n",
      "    * `pip install -U \"tensorflow==1.*\"`\n",
      "\n",
      "  Otherwise your code may be broken by the change.\n",
      "\n",
      "  \n",
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from skimage.exposure import adjust_gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.legacy import interfaces\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "class AdamAccumulate(Optimizer):\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, accum_iters=1, **kwargs):\n",
    "        if accum_iters < 1:\n",
    "            raise ValueError('accum_iters must be >= 1')\n",
    "        super(AdamAccumulate, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n",
    "        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        completed_updates = K.cast(K.tf.floordiv(self.iterations, self.accum_iters), K.floatx())\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * completed_updates))\n",
    "\n",
    "        t = completed_updates + 1\n",
    "\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        # self.iterations incremented after processing a batch\n",
    "        # batch:              1 2 3 4 5 6 7 8 9\n",
    "        # self.iterations:    0 1 2 3 4 5 6 7 8\n",
    "        # update_switch = 1:        x       x    (if accum_iters=4)  \n",
    "        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n",
    "        update_switch = K.cast(update_switch, K.floatx())\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n",
    "\n",
    "            sum_grad = tg + g\n",
    "            avg_grad = sum_grad / self.accum_iters_float\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n",
    "\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n",
    "            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n",
    "            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(AdamAccumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(probability, threshold, min_size):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((350, 525), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22184, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>hasMask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Flower</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Gravel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels  \\\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...   \n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...   \n",
       "2  0011165.jpg_Gravel                                                NaN   \n",
       "3   0011165.jpg_Sugar                                                NaN   \n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23...   \n",
       "\n",
       "       ImageId ClassId  hasMask  \n",
       "0  0011165.jpg    Fish     True  \n",
       "1  0011165.jpg  Flower     True  \n",
       "2  0011165.jpg  Gravel    False  \n",
       "3  0011165.jpg   Sugar    False  \n",
       "4  002be4f.jpg    Fish     True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df['ImageId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "train_df['ClassId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "train_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('sample_submission.csv')\n",
    "sub_df['ImageId'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "test_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5546, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>hasMask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>24dd99c.jpg</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>562f80a.jpg</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>944f10b.jpg</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>55b539f.jpg</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>cdf7242.jpg</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ImageId  hasMask\n",
       "821   24dd99c.jpg      4.0\n",
       "1885  562f80a.jpg      4.0\n",
       "3260  944f10b.jpg      4.0\n",
       "1872  55b539f.jpg      4.0\n",
       "4464  cdf7242.jpg      4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\n",
    "mask_count_df.sort_values('hasMask', ascending=False, inplace=True)\n",
    "print(mask_count_df.shape)\n",
    "mask_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_resize(img, input_shape):\n",
    "    \"\"\"\n",
    "    Reshape a numpy array, which is input_shape=(height, width), \n",
    "    as opposed to input_shape=(width, height) for cv2\n",
    "    \"\"\"\n",
    "    height, width = input_shape\n",
    "    return cv2.resize(img, (width, height))\n",
    "    \n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    \n",
    "    mask= np.zeros( width*height ).astype(np.uint8)\n",
    "    \n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "        \n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def build_masks(rles, input_shape, reshape=None):\n",
    "    depth = len(rles)\n",
    "    if reshape is None:\n",
    "        masks = np.zeros((*input_shape, depth))\n",
    "    else:\n",
    "        masks = np.zeros((*reshape, depth))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            if reshape is None:\n",
    "                masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "            else:\n",
    "                mask = rle2mask(rle, input_shape)\n",
    "                reshaped_mask = np_resize(mask, reshape)\n",
    "                masks[:, :, i] = reshaped_mask\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def build_rles(masks, reshape=None):\n",
    "    width, height, depth = masks.shape\n",
    "    \n",
    "    rles = []\n",
    "    \n",
    "    for i in range(depth):\n",
    "        mask = masks[:, :, i]\n",
    "        \n",
    "        if reshape:\n",
    "            mask = mask.astype(np.float32)\n",
    "            mask = np_resize(mask, reshape).astype(np.int64)\n",
    "        \n",
    "        rle = mask2rle(mask)\n",
    "        rles.append(rle)\n",
    "        \n",
    "    return rles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_cross_entropy(beta):\n",
    "    def convert_to_logits(y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        return tf.log(y_pred / (1 - y_pred))\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = convert_to_logits(y_pred)\n",
    "        pos_weight = beta / (1 - beta)\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=pos_weight)\n",
    "        return tf.reduce_mean(loss * (1 - beta))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n",
    "                 base_path='./train_images',\n",
    "                 batch_size=32, dim=(1400, 2100), n_channels=3, reshape=None, gamma=None,\n",
    "                 augment=False, n_classes=4, random_state=2019, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.base_path = base_path\n",
    "        self.target_df = target_df\n",
    "        self.list_IDs = list_IDs\n",
    "        self.reshape = reshape\n",
    "        self.gamma = gamma\n",
    "        self.n_channels = n_channels\n",
    "        self.augment = augment\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self.__generate_X(list_IDs_batch)\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "            y = self.__generate_y(list_IDs_batch)\n",
    "            \n",
    "            if self.augment:\n",
    "                X, y = self.__augment_batch(X, y)\n",
    "            \n",
    "            return X, y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "\n",
    "        else:\n",
    "            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(self.random_state)\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generate_X(self, list_IDs_batch):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        if self.reshape is None:\n",
    "            X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        else:\n",
    "            X = np.empty((self.batch_size, *self.reshape, self.n_channels))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['ImageId'].iloc[ID]\n",
    "            img_path = f\"{self.base_path}/{im_name}\"\n",
    "            img = self.__load_rgb(img_path)\n",
    "            \n",
    "            if self.reshape is not None:\n",
    "                img = np_resize(img, self.reshape)\n",
    "            \n",
    "            # Adjust gamma\n",
    "            if self.gamma is not None:\n",
    "                img = adjust_gamma(img, gamma=self.gamma)\n",
    "            \n",
    "            # Store samples\n",
    "            X[i,] = img\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __generate_y(self, list_IDs_batch):\n",
    "        if self.reshape is None:\n",
    "            y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n",
    "        else:\n",
    "            y = np.empty((self.batch_size, *self.reshape, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['ImageId'].iloc[ID]\n",
    "            image_df = self.target_df[self.target_df['ImageId'] == im_name]\n",
    "            \n",
    "            rles = image_df['EncodedPixels'].values\n",
    "            \n",
    "            if self.reshape is not None:\n",
    "                masks = build_masks(rles, input_shape=self.dim, reshape=self.reshape)\n",
    "            else:\n",
    "                masks = build_masks(rles, input_shape=self.dim)\n",
    "            \n",
    "            y[i, ] = masks\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def __load_grayscale(self, img_path):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def __load_rgb(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def __random_transform(self, img, masks):\n",
    "        composition = albu.Compose([\n",
    "            albu.HorizontalFlip(),\n",
    "            albu.VerticalFlip(),\n",
    "            albu.ShiftScaleRotate(rotate_limit=30, shift_limit=0.1)\n",
    "        ])\n",
    "        \n",
    "        composed = composition(image=img, mask=masks)\n",
    "        aug_img = composed['image']\n",
    "        aug_masks = composed['mask']\n",
    "        \n",
    "        return aug_img, aug_masks\n",
    "    \n",
    "    def __augment_batch(self, img_batch, masks_batch):\n",
    "        for i in range(img_batch.shape[0]):\n",
    "            img_batch[i, ], masks_batch[i, ] = self.__random_transform(\n",
    "                img_batch[i, ], masks_batch[i, ])\n",
    "        \n",
    "        return img_batch, masks_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    mask_count_df.index, random_state=2019, test_size=0.2\n",
    ")\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    train_idx, \n",
    "    df=mask_count_df,\n",
    "    target_df=train_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reshape=(320, 480),\n",
    "    gamma=0.8,\n",
    "    augment=True,\n",
    "    n_channels=3,\n",
    "    n_classes=4\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_idx, \n",
    "    df=mask_count_df,\n",
    "    target_df=train_df,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    reshape=(320, 480),\n",
    "    gamma=0.8,\n",
    "    augment=False,\n",
    "    n_channels=3,\n",
    "    n_classes=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               (None, 320, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 320, 480, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 326, 486, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 160, 240, 64) 9408        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 160, 240, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 160, 240, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 162, 242, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 80, 120, 64)  0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 80, 120, 64)  256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 80, 120, 64)  0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 82, 122, 64)  0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 80, 120, 64)  36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 80, 120, 64)  256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 80, 120, 64)  0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 82, 122, 64)  0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 80, 120, 64)  36864       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 80, 120, 64)  4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 80, 120, 64)  0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 80, 120, 64)  256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 80, 120, 64)  0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 82, 122, 64)  0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 80, 120, 64)  36864       zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 80, 120, 64)  256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 80, 120, 64)  0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 82, 122, 64)  0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 80, 120, 64)  36864       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 80, 120, 64)  0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 80, 120, 64)  256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 80, 120, 64)  0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 82, 122, 64)  0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 40, 60, 128)  73728       zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 40, 60, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 40, 60, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 42, 62, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 40, 60, 128)  147456      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 40, 60, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 40, 60, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 40, 60, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 40, 60, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 42, 62, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 40, 60, 128)  147456      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 40, 60, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 40, 60, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 42, 62, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 40, 60, 128)  147456      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 40, 60, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 40, 60, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 40, 60, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 42, 62, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 20, 30, 256)  294912      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 20, 30, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 20, 30, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 22, 32, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 20, 30, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 20, 30, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 20, 30, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 20, 30, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 20, 30, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 22, 32, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 20, 30, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 20, 30, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 20, 30, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 22, 32, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 20, 30, 256)  589824      zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 20, 30, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 20, 30, 256)  1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 20, 30, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 22, 32, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 10, 15, 512)  1179648     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 10, 15, 512)  2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 10, 15, 512)  0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 12, 17, 512)  0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 10, 15, 512)  2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 10, 15, 512)  131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 10, 15, 512)  0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 10, 15, 512)  2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 10, 15, 512)  0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 12, 17, 512)  0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 10, 15, 512)  2359296     zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 10, 15, 512)  2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 10, 15, 512)  0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPadding2 (None, 12, 17, 512)  0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 10, 15, 512)  2359296     zero_padding2d_18[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 10, 15, 512)  0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 10, 15, 512)  2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 10, 15, 512)  0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_upsampling (UpSa (None, 20, 30, 512)  0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_concat (Concaten (None, 20, 30, 768)  0           decoder_stage0_upsampling[0][0]  \n",
      "                                                                 stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_conv (Conv2D)   (None, 20, 30, 256)  1769472     decoder_stage0_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_bn (BatchNormal (None, 20, 30, 256)  1024        decoder_stage0a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_relu (Activatio (None, 20, 30, 256)  0           decoder_stage0a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_conv (Conv2D)   (None, 20, 30, 256)  589824      decoder_stage0a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_bn (BatchNormal (None, 20, 30, 256)  1024        decoder_stage0b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_relu (Activatio (None, 20, 30, 256)  0           decoder_stage0b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_upsampling (UpSa (None, 40, 60, 256)  0           decoder_stage0b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_concat (Concaten (None, 40, 60, 384)  0           decoder_stage1_upsampling[0][0]  \n",
      "                                                                 stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_conv (Conv2D)   (None, 40, 60, 128)  442368      decoder_stage1_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_bn (BatchNormal (None, 40, 60, 128)  512         decoder_stage1a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_relu (Activatio (None, 40, 60, 128)  0           decoder_stage1a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_conv (Conv2D)   (None, 40, 60, 128)  147456      decoder_stage1a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_bn (BatchNormal (None, 40, 60, 128)  512         decoder_stage1b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_relu (Activatio (None, 40, 60, 128)  0           decoder_stage1b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_upsampling (UpSa (None, 80, 120, 128) 0           decoder_stage1b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_concat (Concaten (None, 80, 120, 192) 0           decoder_stage2_upsampling[0][0]  \n",
      "                                                                 stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_conv (Conv2D)   (None, 80, 120, 64)  110592      decoder_stage2_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_bn (BatchNormal (None, 80, 120, 64)  256         decoder_stage2a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_relu (Activatio (None, 80, 120, 64)  0           decoder_stage2a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_conv (Conv2D)   (None, 80, 120, 64)  36864       decoder_stage2a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_bn (BatchNormal (None, 80, 120, 64)  256         decoder_stage2b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_relu (Activatio (None, 80, 120, 64)  0           decoder_stage2b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_upsampling (UpSa (None, 160, 240, 64) 0           decoder_stage2b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_concat (Concaten (None, 160, 240, 128 0           decoder_stage3_upsampling[0][0]  \n",
      "                                                                 relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_conv (Conv2D)   (None, 160, 240, 32) 36864       decoder_stage3_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_bn (BatchNormal (None, 160, 240, 32) 128         decoder_stage3a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_relu (Activatio (None, 160, 240, 32) 0           decoder_stage3a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_conv (Conv2D)   (None, 160, 240, 32) 9216        decoder_stage3a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_bn (BatchNormal (None, 160, 240, 32) 128         decoder_stage3b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_relu (Activatio (None, 160, 240, 32) 0           decoder_stage3b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_upsampling (UpSa (None, 320, 480, 32) 0           decoder_stage3b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_conv (Conv2D)   (None, 320, 480, 16) 4608        decoder_stage4_upsampling[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_bn (BatchNormal (None, 320, 480, 16) 64          decoder_stage4a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_relu (Activatio (None, 320, 480, 16) 0           decoder_stage4a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_conv (Conv2D)   (None, 320, 480, 16) 2304        decoder_stage4a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_bn (BatchNormal (None, 320, 480, 16) 64          decoder_stage4b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_relu (Activatio (None, 320, 480, 16) 0           decoder_stage4b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv2D)             (None, 320, 480, 4)  580         decoder_stage4b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Activation)            (None, 320, 480, 4)  0           final_conv[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,341,005\n",
      "Trainable params: 14,331,079\n",
      "Non-trainable params: 9,926\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = AdamAccumulate(lr=0.002, accum_iters=8)\n",
    "\n",
    "model = sm.Unet(\n",
    "    'resnet18', \n",
    "    classes=4,\n",
    "    input_shape=(320, 480, 3),\n",
    "    activation='sigmoid'\n",
    ")\n",
    "model.compile(optimizer=opt, loss=bce_dice_loss, metrics=[dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  7/138 [>.............................] - ETA: 3:22:24 - loss: 1.6780 - dice_coef: 0.2288"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_dice_coef', min_delta=0.0001, patience=5, verbose=1, mode='max', restore_best_weights=True)\n",
    "rlr = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.2, patience=2, verbose=1, mode='max', min_delta=0.0001)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint, rlr, es],\n",
    "    epochs=30,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
